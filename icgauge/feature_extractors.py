# -*- coding: utf-8 -*-
#!/usr/bin/python

from collections import Counter
from nltk.tokenize import word_tokenize
import numpy as np
import re

import utils_wordlists

# Note: Best to use independent namespaces for each key,
# since multiple feature functions can be grouped together.

def manual_content_flags(paragraph):
    """
    Baseline feature extractor, based on manual. Produces a feature function 
    that detects the presence of the example "content flag" phrases in the 
    coding manual.
    
    Parameters
    ----------
    paragraph : string
        Content string from which features should be extracted.
              
    Returns
    -------
    dict : string -> integer
        The number of times a flag at each level occurred in the text 
        (e.g., {"flag_1": 3, "flag_2": 1} )
      
    """
    flags = {"flag_1": utils_wordlists.get_manual_flags(1),
             "flag_2": utils_wordlists.get_manual_flags(2),
             "flag_3": utils_wordlists.get_manual_flags(3),
             "flag_4": utils_wordlists.get_manual_flags(4),
             "flag_5": utils_wordlists.get_manual_flags(5),
            } 
    
    feature_presence = Counter()
    tokenized_and_lowercase = word_tokenize(paragraph.lower())
    for label, set_of_flags in flags.iteritems():
        for flag in set_of_flags:
            if flag in tokenized_and_lowercase:
                feature_presence[label] += 1

    return feature_presence
    
def unigrams(paragraph):
    """Produces a feature function on unigrams."""
    return Counter(word_tokenize(paragraph))

def length(paragraph):
    """
    Produces length-related features:
        - number of characters
        - number of white-space separated words (tokens)
        - mean length of white-space separated tokens
    """
    tokens = word_tokenize(paragraph)
    result = Counter()
    result["length_in_characters"] = len(paragraph)
    result["length_in_words"] = len(tokens)
    result["length_mean_word_len"] = np.mean([len(t) for t in tokens])
    return result
    
def wordlist_presence(wordlist_func, paragraph):
    """
    Produces feature list that is:
        - count of each word in the list generated by wordlist_func
        - count of tokens
        - count of types
        - proportion of tokens in paragraph that are in that list
    using a lower-case version of the original paragraph and a version of
    the paragraph in which all the tokens are separated by spaces (to 
    address cases where morphological forms are attached -- e.g.,
    "wouldn't" --> "would n't")
    """
    presence = Counter()
    paragraph = paragraph.lower()
    reconstituted_paragraph = " ".join(word_tokenize(paragraph))
    
    wordlist = wordlist_func()
    for phrase in wordlist:
        matcher = re.compile(r'\b({0})\b'.format(phrase), flags=re.IGNORECASE)
        matches = matcher.findall(paragraph)
        if len(matches) > 0:
            presence[phrase] += len(matches)
        else:
            matches = matcher.findall(reconstituted_paragraph)
            if len(matches) > 0:
                presence[phrase] += len(matches)
    
    return presence

def modal_presence(paragraph):
    modals = wordlist_presence(utils_wordlists.get_modals, paragraph)
    tokens = word_tokenize(paragraph)
    
    modals["modal_count_token"] = np.sum( \
        [value for key, value in modals.items()])    
    modals["modal_count_type"] = len(modals) - 1 # -1 bc *_count_token
    modals["modal_freq"] = 1.0*modals["modal_count_token"] / len(tokens)
    
    return modals
    
def hedge_presence(paragraph):
    hedges = wordlist_presence(utils_wordlists.get_hedges, paragraph)
    hedges["hedge_count_token"] = np.sum( \
        [value for key, value in hedges.items()])    
    hedges["hedge_count_type"] = len(hedges) - 1 # -1 bc *_count_token
    return hedges
    
def conjunctives_presence(paragraph):
    conjunctives = wordlist_presence(utils_wordlists.get_conjunctives, paragraph)
    conjunctives["conjunctive_count_token"] = np.sum( \
        [value for key, value in conjunctives.items()])    
    conjunctives["conjunctive_count_type"] = len(conjunctives) - 1 # -1 bc *_count_token
    return conjunctives

def punctuation_presence(paragraph):
    punctuation = utils_wordlists.get_punctuation()
    tokens = word_tokenize(paragraph.lower())
    
    result = Counter()
    for mark in punctuation:
        ct = tokens.count(mark)
        if ct > 0:
            result[mark] = ct
            
    result["punctuation_count_token"] = np.sum( \
        [value for key, value in result.items()])    
    result["punctuation_count_type"] = len(result) - 1 # -1 bc *_count_token
    result["punctuation_freq"] = 1.0*result["punctuation_count_token"] / len(tokens)
    
    return result
    
\
# Other potentially useful:
# - syntactic: passive voice
# - discourse: argument structure (statement-assessment as derived from but/and/because), 
#   sentiment, old->new information
# - lexical: modal verbs, but/and/because, presence of hedges
# - morphological: -ly on adjectives of degree