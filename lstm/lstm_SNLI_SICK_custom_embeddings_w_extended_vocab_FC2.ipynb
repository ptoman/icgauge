{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting the paths\n",
    "percentage_split = .7\n",
    "num_epoch = 10\n",
    "saved_model_name = \"SNLI_SICK_custom_embeddings_w_extended_vocab_FC2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "word_count = 1\n",
    "\n",
    "def parse_data(json_data):\n",
    "    global word_count\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    for d in json_data:\n",
    "        current_attribute_list = []\n",
    "        words = tokenized_and_lowercase = word_tokenize(d['example'].lower())\n",
    "        for w in words:\n",
    "            if w not in vocab:\n",
    "                vocab[w] = word_count\n",
    "                word_count += 1\n",
    "            current_attribute_list.append(vocab[w])\n",
    "        X.append(current_attribute_list)\n",
    "        Y.append(d['label'])\n",
    "\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawDataPath = raw_filepath + \"raw_data.txt\"\n",
    "\n",
    "max_length = -1;\n",
    "with open(rawDataPath) as f:\n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    for l in lines:\n",
    "        words = word_tokenize(l)\n",
    "        for w in words:\n",
    "            if w not in vocab:\n",
    "                vocab[w] = word_count\n",
    "                word_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(filepath) as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "    X, Y = parse_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of examples:\", len(X))\n",
    "print(\"Number of distinct words:\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_length_list = [len(eg) for eg in X]\n",
    "num_words_in_longest_sentence = max(data_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Length of the biggest sentence:\", num_words_in_longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_words_in_longest_sentence = 512 # since length of the largest paragraph is 392. Now 1.5 * 392 ~ 494. 512 closest\n",
    "num_training_examples = int(math.ceil(len(X) * percentage_split))\n",
    "print(num_training_examples)\n",
    "trainX = X[:num_training_examples]\n",
    "trainY = Y[:num_training_examples]\n",
    "\n",
    "testX = X[num_training_examples:]\n",
    "testY = Y[num_training_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Sequence padding \n",
    "trainX = pad_sequences(trainX, maxlen=num_words_in_longest_sentence, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=num_words_in_longest_sentence, value=0.)\n",
    "\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "testY = to_categorical(testY, nb_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "print(\"num_words_in_longest_sentence:\", num_words_in_longest_sentence)\n",
    "layer_input = tflearn.input_data([None, num_words_in_longest_sentence])\n",
    "# embedding = tflearn.embedding(layer_input, input_dim=word_count, output_dim=128)\n",
    "embedding = tflearn.embedding(layer_input, input_dim=word_count, output_dim=128, name='my_embedding')\n",
    "lstm = tflearn.lstm(embedding, 128)\n",
    "dropout = tflearn.dropout(lstm, 0.5)\n",
    "softmax = tflearn.fully_connected(dropout, 2, activation='softmax')\n",
    "net = tflearn.regression(softmax, optimizer='adam',\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "model = tflearn.DNN(net, clip_gradients=0., tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, n_epoch=num_epoch,validation_set=(testX, testY), show_metric=True,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(saved_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dumping the vocab\n",
    "with open('lstm_vocab_with_augmented_vocab','w') as f:\n",
    "    pickle.dump(vocab,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tflearn.DNN(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load(saved_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
