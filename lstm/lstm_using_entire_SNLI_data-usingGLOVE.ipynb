{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import json\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting the paths\n",
    "filepath = '/home/pujun/Desktop/StanfordClasses/lstm for natural language understanding/hi.json'\n",
    "percentage_split = .7\n",
    "GLOVE_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path_to_glove = os.environ.get(\"GLV_HOME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glove2dict(src_filename):\n",
    "    \"\"\"GloVe Reader.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the GloVe file to be processed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping words to their GloVe vectors.\n",
    "    \n",
    "    \"\"\"\n",
    "    reader = csv.reader(open(src_filename), delimiter=' ', quoting=csv.QUOTE_NONE)    \n",
    "    return {line[0]: np.array(list(map(float, line[1: ]))) for line in reader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove = glove2dict(os.path.join(path_to_glove, \n",
    "            'glove.6B.%dd.txt' % GLOVE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "def parse_data_using_glove(json_data, num_examples_to_read=10000, num_words_in_longest_sentence=82):\n",
    "    Y = []\n",
    "    \n",
    "    X = np.random.rand(num_examples_to_read, 82, 50)\n",
    "    for i, d in enumerate(json_data):\n",
    "        if i >= num_examples_to_read:\n",
    "            break\n",
    "        current_attribute_list = np.random.rand(82, 50)\n",
    "        tokenized_and_lowercase = word_tokenize(d['example'].lower())\n",
    "        for j,w in enumerate(tokenized_and_lowercase):\n",
    "            current_attribute_list[j,:] = np.array(glove.get(w))\n",
    "            vocab.add(w)\n",
    "\n",
    "        for j in range(len(tokenized_and_lowercase), num_words_in_longest_sentence):\n",
    "            current_attribute_list[j,:] = np.zeros(50);\n",
    "        \n",
    "        X[i,:, :] = current_attribute_list\n",
    "        Y.append(d['label'])\n",
    "\n",
    "    return (X, np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5430\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(filepath) as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "    X, Y = parse_data_using_glove(data)\n",
    "word_count = len(vocab)\n",
    "print(\"Vocab size:\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 82, 50)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print (np.array(X)[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_length_list = [len(eg) for eg in X]\n",
    "num_words_in_longest_sentence = max(data_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the biggest sentence: 82\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the biggest sentence:\", num_words_in_longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 7000\n",
      "(7000, 82, 50)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "num_training_examples = int(math.ceil(len(X) * percentage_split))\n",
    "print(\"number of training examples:\", num_training_examples)\n",
    "npX = np.array(X)\n",
    "npY = np.array(Y)\n",
    "trainX = npX[:num_training_examples]\n",
    "trainY = npY[:num_training_examples]\n",
    "\n",
    "testX = X[num_training_examples:]\n",
    "testY = Y[num_training_examples:]\n",
    "\n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words_in_longest_sentence: 82\n",
      "GLOVE_SIZE: 50\n",
      "word_count 5430\n",
      "dimension of X 10000 where each element is 4100\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "# Sequence padding \n",
    "# trainX = pad_sequences(trainX, maxlen=num_words_in_longest_sentence, value=0.)\n",
    "# testX = pad_sequences(testX, maxlen=num_words_in_longest_sentence, value=0.)\n",
    "\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY, nb_classes=2)\n",
    "testY = to_categorical(testY, nb_classes=2)\n",
    "word_count = len(vocab)\n",
    "print(\"num_words_in_longest_sentence:\", num_words_in_longest_sentence)\n",
    "print(\"GLOVE_SIZE:\", GLOVE_SIZE)\n",
    "print(\"word_count\",  word_count)\n",
    "print(\"dimension of X\", len(X), \"where each element is\", X[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "net = tflearn.input_data(shape=[None, 82, 50],name='input')\n",
    "net = tflearn.lstm(net, 82, return_seq=True)\n",
    "net = tflearn.dropout(net,0.5)\n",
    "net = tflearn.lstm(net, 82)\n",
    "net = tflearn.dropout(net,0.5)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam',\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 550  | total loss: \u001b[1m\u001b[32mnan\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: nan - acc: 0.4969 | val_loss: nan - val_acc: 0.5000 -- iter: 7000/7000\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32mnan\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 010 | loss: nan - acc: 0.4969 | val_loss: nan - val_acc: 0.5000 -- iter: 7000/7000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = tflearn.DNN(net, clip_gradients=0., tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(trainX, trainY), show_metric=True,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
